# -*- coding: utf-8 -*-
"""creditcardfrauddetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MuhammadRuby/Crdit_Card_Fraud_Detection/blob/main/creditcardfrauddetection.ipynb

<a href="https://www.kaggle.com/code/muhammadrubymuhammad/creditcardfrauddetection?scriptVersionId=199660855" target="_blank"><img align="left" alt="Kaggle" title="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"></a>

hello guys.
                                                         Credit Card Fraud Detection
Credit card fraud is a significant issue in the financial industry, costing billions of dollars annually. Fraudulent transactions can occur in various forms, such as unauthorized purchases, identity theft, and account takeovers. Detecting these fraudulent activities promptly is crucial to minimize financial losses and protect consumers.

![image.png](attachment:2ecd82a0-d1d6-464b-ac48-dd924b7fc64c.png)

> **now we will make a model to detect if transaction is normal or fraud**

# 1) Import Libraries
"""

import pandas as pd  # Used for data analysis and manipulation (loading data from CSV, dataframes, etc.)
import numpy as np  # Provides numerical operations and array functionalities
import matplotlib.pyplot as plt  # Used for creating visualizations (plots, charts, etc.)
plt.style.use('ggplot')  # Sets a nice default style for plots using 'ggplot' theme
import seaborn as sns  # Builds on top of matplotlib for creating statistical data visualizations

from sklearn.preprocessing import StandardScaler  # Used to standardize numerical features for machine learning models
from sklearn.model_selection import train_test_split  # Splits data into training and testing sets for model evaluation
from sklearn.linear_model import LogisticRegression  # Implements logistic regression algorithm for classification tasks
from sklearn.ensemble import RandomForestClassifier  # Implements random forest algorithm for classification
from sklearn.tree import DecisionTreeClassifier  # Implements decision tree algorithm for classification
from imblearn.over_sampling import SMOTE # resampling data
import joblib

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  # Provides various metrics to evaluate model performance
import warnings
warnings.filterwarnings('ignore')  # Suppresses warnings (use with caution, consider addressing the warnings)

"""# 2) Load and Explore the Dataset

## 1) read data
"""

path = '/kaggle/input/creditcardfraud/creditcard.csv' # you will need to change this debondeing on path o data
data = pd.read_csv(path)

"""## 2) Explaor our data

### 1) overview on data
"""

data.head() #will show first 5 column of data

data .tail() #will show last 5 column of data

"""**note** (if you run this in jupter or Colab notebook )as you can see above some column not appear as a value in display you can solve it using this command :

"""

# remove comment to run this code if you need that
# pd.options.display.max_columns = None # to show all columns in display

data.info() #give us some information about data

data.describe() #show overview about data

data.columns

print(f'Number of rows: {data.shape[0]}')
print(f'Number of columns: {data.shape[1]}')

"""### 2) Data cleaning"""

data.isnull().sum()

"""great, no nulls in data"""

data.duplicated().value_counts()

"""**Note :**

We have some duplicate data in it, and we're going to drop it now because it's not necessary in our data
"""

data.drop_duplicates(inplace=True) # to drop duplicates

data.duplicated().value_counts() # test again if any duplicates

"""### 3) Data Visualization"""

# Histogram of Amount to Understand the distribution of transaction amounts.
sns.histplot(data['Amount'], bins=50, kde=True)
plt.title('Histogram of Transaction Amounts')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.show()

# Visualize the proportion of fraudulent and non-fraudulent transactions.
sns.countplot(x='Class', data=data)
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# Compare the distribution of transaction amounts for fraudulent and non-fraudulent transactions.
sns.boxplot(x='Class', y='Amount', data=data)
plt.title('Box Plots of Amount by Class')
plt.xlabel('Class')
plt.ylabel('Amount')
plt.show()

"""### 4) Class Imbalance"""

fraud_count = data['Class'].value_counts()[1]
non_fraud_count = data['Class'].value_counts()[0]

print("Number of fraudulent transactions:", fraud_count)
print("Number of non-fraudulent transactions:", non_fraud_count)

imbalance_ratio = non_fraud_count / fraud_count
print("Class imbalance ratio:", imbalance_ratio)

"""## 3) Feature Engineering

### 1) feature scaling
"""

data['Amount'].plot.kde()

# As we saw earlier, the Amount is not in the same range as the data

# we will use Standered scaler to fix range of values in Amount columns
Scaler = StandardScaler()
data['Amount'] = Scaler.fit_transform(pd.DataFrame(data['Amount']))

data.head()

"""> **column Time not necessary**"""

data.drop('Time', axis=1, inplace=True)

"""# 4) spliting data

"""

X = data.drop('Class', axis=1)
y = data['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# 5)Model Selection"""

# make all algorithms as a dict to loop in all Classifer
Classifier = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}

for name, clf in Classifier.items() :
  print (f'============= {name} ==================')
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  print(f'Accuracy = {accuracy_score(y_test, y_pred)}\n')
  print(f'precision_score = {precision_score(y_test, y_pred)}\n')
  print(f'recall_score = {recall_score(y_test, y_pred)}\n')
  print(f'f1_score = {f1_score(y_test, y_pred)}\n')
  print(f'confusion_matrix = {confusion_matrix(y_test, y_pred)}\n')

"""> **As you can see, the accuracy of the model is not the best, due to the presence of an imbalance in the data**

# 6) Handling Imbalanced Data
"""

legit = data[data.Class == 0]
fraud = data[data.Class == 1]

plt.pie(data['Class'].value_counts(), labels=['Legit', 'Fraud'])

print(f'Legit >> {legit.shape}')
print(f'Fraud >> {fraud.shape}')

"""## 1) Undersampling the Majority Class"""

# we get a sample from data to make data balanced
new_data = pd.concat([legit.sample(n=473), fraud], ignore_index = True)

new_data['Class'].value_counts()

"""> spliting data again depond on new version of data"""

X_undersample = new_data.drop('Class', axis=1)
y_undersample = new_data['Class']

X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample, test_size=0.2, random_state=42, shuffle=True)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""> choose model"""

Classifier = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}

for name, clf in Classifier.items() :
  print (f'============= {name} ==================')
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  print(f'Accuracy = {accuracy_score(y_test, y_pred)}\n')
  print(f'precision_score = {precision_score(y_test, y_pred)}\n')
  print(f'recall_score = {recall_score(y_test, y_pred)}\n')
  print(f'f1_score = {f1_score(y_test, y_pred)}\n')
  print(f'confusion_matrix = {confusion_matrix(y_test, y_pred)}\n')

"""## 2) Oversampling the Minority Class"""

smote = SMOTE(sampling_strategy='minority')
X_sm, y_sm = smote.fit_resample(X, y)

y_sm.value_counts()

"""> splinting data with update"""

X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=42, shuffle=True)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""> choose model with update"""

Classifier = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}

for name, clf in Classifier.items() :
  print (f'============= {name} ==================')
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  print(f'Accuracy = {accuracy_score(y_test, y_pred)}\n')
  print(f'precision_score = {precision_score(y_test, y_pred)}\n')
  print(f'recall_score = {recall_score(y_test, y_pred)}\n')
  print(f'f1_score = {f1_score(y_test, y_pred)}\n')
  print(f'confusion_matrix = {confusion_matrix(y_test, y_pred)}\n')

"""> **The Random Forest model is the best in terms of performance:**
>
> It has the highest accuracy (99.99%).

> Perfect recall (1.0) meaning no true positives are missed.

> Precision and F1-score are also nearly perfect.

# 7) Deploying the Model
"""

RandomForestModel = RandomForestClassifier()
RandomForestModel.fit(X_train, y_train)

joblib.dump(RandomForestModel, 'CerditCardRandomForestModel.pkl')

# get example from data to test on it our model
print (list(data.iloc[540])) # iloc to get row using index location

# load model when put it in app as this :
FinalModel = joblib.load('CerditCardRandomForestModel.pkl')

Modelpredtest = FinalModel.predict([[-0.960818700152674, 0.0710130174771463, 2.60281933663759, -0.99869306098515, -0.383880011575843, -0.208286115073412, 0.368938158535082, 0.0595136407483677, 0.504836275517028, -0.678021319743396, 1.05047512024654, 1.31612484967149, 0.377964606253002, -0.861676968779237, -1.75580529833207, 0.097838076187801, -0.4651699009842, -0.224919085009194, -0.0197062093199049, 0.315315447416209, -0.13682682244554, -0.0813335069826737, -0.016162266594744, 0.605342437117432, -0.12175945402901, 0.736684428025349, 0.0771085656054282, -0.069752043247462, -0.12932434973089274]])

if Modelpredtest[0] == 1 :
    print("\n This is fraudulent transactions\n")
else :
    print("\nThis is  non-fraudulent transactions\n")

"""# In this project, we successfully developed a credit card fraud detection system using machine learning techniques.

## Conclusion for the Project:

In this project, we aimed to evaluate the performance of multiple machine learning models to identify the best classifier for a given dataset.
Three models were trained and tested: Logistic Regression, Decision Tree, and Random Forest, with their performances measured using key metrics such as
accuracy, precision, recall, and F1-score.

Through a detailed analysis of the results:

Logistic Regression provided solid performance, but it was outperformed by the more complex models, particularly in recall, where it missed more positive cases.

The Decision Tree model significantly improved over logistic regression, achieving high accuracy and precision, and reducing both false positives and false negatives.

Random Forest emerged as the best model, exhibiting near-perfect scores across all metrics. Its accuracy of 99.99% and perfect recall (1.0) made it  highly reliable for detecting all positive cases with minimal false positives.

Given the results, Random Forest was chosen as the final model for deployment due to its ability to balance both precision and recall, ensuring high  performance in real-world scenarios.


## Key Outcomes:
High Accuracy: The Random Forest model achieved outstanding accuracy, minimizing errors and making reliable predictions.
Strong Precision and Recall: It identified true positives without sacrificing precision, resulting in an F1-score close to perfection.
Real-world Application: The chosen model is suitable for deployment in environments where minimizing false positives and negatives is critical, such as medical diagnosis, fraud detection, or other high-stakes classifications.

The final deployment using the Random Forest model ensures that the system can handle real-time data input and provide robust, reliable predictions.

# If you have any specific questions or need help with something, feel free to ask. Whether itâ€™s about Python programming, data science, machine learning, or anything else, Iâ€™m here to help. ðŸ˜Š
## [Linkedin](https://www.linkedin.com/in/muhammad-ruby-muhammad)
## [github](https://github.com/MuhammadRuby)
## [Mail](mohamedroby831@gmail.com)
"""